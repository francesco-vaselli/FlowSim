# mixture_flow_model_config.yaml

input_dim: 16  # Dimensionality of input (x)
context_dim: 12  # Dimensionality of context (y)
gpu : 0 # GPU to use
train_kwargs:
  epochs: 1000  # Number of epochs
  lr: 0.001  # Learning rate
  optimizer: "adam"  # Optimizer
  log_name: "CRT_weighted_inputs_loss_divide_by_max"  # Log directory NOTE: this will be overwritten if NAME IS THE SAME
  resume: false  # Resume training
  resume_checkpoint: null # Checkpoint to resume from
  save_freq: 10  # Save frequency
  eval_freq: 10  # Evaluation frequency
  scheduler: "ReduceLROnPlateau"  # Learning rate scheduler
data_kwargs:
  dataset_path : "../data/gen_ttbar_2M_final-001.npy"
  N_train: 300000  # Number of samples
  N_test: 200000  # Number of test samples
  batch_size: 256  # Batch size (increase to 4k for cfm)
  test_batch_size: 10000  # Test batch size
  flavour_ohe: true  # One-hot encode flavour
  standardize: true  # Standardize data
  noise_distribution: "gaussian"  # Noise distribution, choose from 'gaussian' or 'uniform'
  weights: {
    "btag": 10.0,
    "recoPt": 10.0,
    "recoPhi": 1.0,
    "recoEta": 1.0,
    "recoNConstituents": 1.0,
    "nef": 1.0,
    "nhf": 1.0,
    "cef": 1.0,
    "chf": 1.0,
    "qgl": 10.0,
    "jetId": 1.0,
    "ncharged": 1.0,
    "nneutral": 1.0,
    "ctag": 10.0,
    "nSV": 1.0,
    "recoMass": 1.0,
  }
  apply_weights_on_loss: true  # Apply weights on loss
  apply_weights_on_input: true  # Apply weights on input
base_kwargs:
  cfm: # conditional flow matching
    sigma: 0.0001
    matching_type: "AlphaTTarget"
    ode_backend: "torchdiffeq"  # ODE backend, choose from 'torchdiffeq' or 'torchdyn'
    alpha: 1
    timesteps: 100
    type: 'resnet'
    mlp_hidden_dim: 
      - 32
      - 64
      - 128
      - 128
      - 64
      - 32
    mlp_num_hidden: 5
    mlp_activation: "gelu"
    mlp_dropout: 0.0
    mlp_batch_norm: false

